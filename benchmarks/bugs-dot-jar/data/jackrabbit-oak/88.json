{
    "bug_id": 88,
    "classification": {
        "singleLine": false
    },
    "commit": "90ad50da",
    "failing_tests": [
        "org.apache.jackrabbit.oak.plugins.document.MultiDocumentStoreTest"
    ],
    "files": 2,
    "jira_id": "3634",
    "linesAdd": 53,
    "linesRem": 35,
    "nb_error": 1,
    "nb_failure": 1,
    "nb_skipped": 1,
    "nb_test": 3014,
    "patch": "diff --git a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java\nindex 63304e7c18..ced74dbf20 100644\n--- a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java\n+++ b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/mongo/MongoDocumentStore.java\n@@ -21,7 +21,6 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Date;\n-import java.util.HashMap;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -83,6 +82,8 @@\n import com.mongodb.WriteResult;\n \n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Predicates.notNull;\n+import static com.google.common.collect.Maps.filterValues;\n \n /**\n  * A document store that uses MongoDB as the backend.\n@@ -285,22 +286,9 @@ public CacheInvalidationStats invalidateCache(Iterable<String> keys) {\n                         ids.size(), size);\n             }\n \n-            QueryBuilder query = QueryBuilder.start(Document.ID).in(ids);\n-            // Fetch only the modCount and id\n-            final BasicDBObject fields = new BasicDBObject(Document.ID, 1);\n-            fields.put(Document.MOD_COUNT, 1);\n-            \n-            DBCursor cursor = nodes.find(query.get(), fields);\n-            cursor.setReadPreference(ReadPreference.primary());\n+            Map<String, Number> modCounts = getModCounts(ids);\n             result.queryCount++;\n \n-            Map<String, Number> modCounts = new HashMap<String, Number>();\n-            for (DBObject obj : cursor) {\n-                String id = (String) obj.get(Document.ID);\n-                Number modCount = (Number) obj.get(Document.MOD_COUNT);\n-                modCounts.put(id, modCount);\n-            }\n-\n             int invalidated = nodesCache.invalidateOutdated(modCounts);\n             result.cacheEntriesProcessedCount += modCounts.size();\n             result.invalidationCount += invalidated;\n@@ -906,18 +894,26 @@ boolean canUseModifiedTimeIdx(long modifiedTimeInSecs) {\n             try {\n                 dbCollection.update(query.get(), update, false, true);\n                 if (collection == Collection.NODES) {\n+                    Map<String, Number> modCounts = getModCounts(filterValues(cachedDocs, notNull()).keySet());\n                     // update cache\n                     for (Entry<String, NodeDocument> entry : cachedDocs.entrySet()) {\n                         // the cachedDocs is not empty, so the collection = NODES\n                         Lock lock = nodeLocks.acquire(entry.getKey());\n                         try {\n-                            if (entry.getValue() == null || entry.getValue() == NodeDocument.NULL) {\n+                            Number postUpdateModCount = modCounts.get(entry.getKey());\n+                            if (postUpdateModCount != null\n+                                    && entry.getValue() != null\n+                                    && entry.getValue() != NodeDocument.NULL\n+                                    && (postUpdateModCount.longValue() - 1) == entry.getValue().getModCount()) {\n+                                // post update modCount is one higher than\n+                                // what we currently see in the cache. we can\n+                                // replace the cached document\n+                                NodeDocument newDoc = applyChanges(Collection.NODES, entry.getValue(), updateOp.shallowCopy(entry.getKey()));\n+                                nodesCache.replaceCachedDocument(entry.getValue(), newDoc);\n+                            } else {\n                                 // make sure concurrently loaded document is\n                                 // invalidated\n                                 nodesCache.invalidate(entry.getKey());\n-                            } else {\n-                                NodeDocument newDoc = applyChanges(Collection.NODES, entry.getValue(), updateOp.shallowCopy(entry.getKey()));\n-                                nodesCache.replaceCachedDocument(entry.getValue(), newDoc);\n                             }\n                         } finally {\n                             lock.unlock();\n@@ -925,6 +921,11 @@ boolean canUseModifiedTimeIdx(long modifiedTimeInSecs) {\n                     }\n                 }\n             } catch (MongoException e) {\n+                // some documents may still have been updated\n+                // invalidate all documents affected by this update call\n+                for (String k : keys) {\n+                    nodesCache.invalidate(k);\n+                }\n                 throw DocumentStoreException.convert(e);\n             }\n         } finally {\n@@ -932,6 +933,35 @@ boolean canUseModifiedTimeIdx(long modifiedTimeInSecs) {\n         }\n     }\n \n+    /**\n+     * Returns the {@link Document#MOD_COUNT} value of the documents with the\n+     * given {@code keys}. The returned map will only contain entries for\n+     * existing documents.\n+     *\n+     * @param keys the keys of the documents.\n+     * @return map with key to {@link Document#MOD_COUNT} value mapping.\n+     * @throws MongoException if the call fails\n+     */\n+    @Nonnull\n+    private Map<String, Number> getModCounts(Iterable<String> keys)\n+            throws MongoException {\n+        QueryBuilder query = QueryBuilder.start(Document.ID).in(keys);\n+        // Fetch only the modCount and id\n+        final BasicDBObject fields = new BasicDBObject(Document.ID, 1);\n+        fields.put(Document.MOD_COUNT, 1);\n+\n+        DBCursor cursor = nodes.find(query.get(), fields);\n+        cursor.setReadPreference(ReadPreference.primary());\n+\n+        Map<String, Number> modCounts = Maps.newHashMap();\n+        for (DBObject obj : cursor) {\n+            String id = (String) obj.get(Document.ID);\n+            Number modCount = (Number) obj.get(Document.MOD_COUNT);\n+            modCounts.put(id, modCount);\n+        }\n+        return modCounts;\n+    }\n+\n     DocumentReadPreference getReadPreference(int maxCacheAge){\n         if(maxCacheAge >= 0 && maxCacheAge < maxReplicationLagMillis) {\n             return DocumentReadPreference.PRIMARY;\ndiff --git a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java\nindex f81e1552da..f07a82701d 100755\n--- a/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java\n+++ b/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBDocumentStore.java\n@@ -1261,6 +1261,9 @@ private static void addUpdateCounters(UpdateOp update) {\n                         qc.addKeys(chunkedIds);\n                         seenQueryContext.add(qc);\n                     }\n+                    for (String id : chunkedIds) {\n+                        nodesCache.invalidate(id);\n+                    }\n                 }\n \n                 Connection connection = null;\n@@ -1285,23 +1288,8 @@ private static void addUpdateCounters(UpdateOp update) {\n                                 qc.addKeys(chunkedIds);\n                             }\n                         }\n-                    }\n-                    for (Entry<String, NodeDocument> entry : cachedDocs.entrySet()) {\n-                        T oldDoc = castAsT(entry.getValue());\n-                        String id = entry.getKey();\n-                        Lock lock = locks.acquire(id);\n-                        try {\n-                            if (oldDoc == null) {\n-                                // make sure concurrently loaded document is\n-                                // invalidated\n+                        for (String id : chunkedIds) {\n                             nodesCache.invalidate(id);\n-                            } else {\n-                                addUpdateCounters(update);\n-                                T newDoc = createNewDocument(collection, oldDoc, update);\n-                                nodesCache.replaceCachedDocument((NodeDocument) oldDoc, (NodeDocument) newDoc);\n-                            }\n-                        } finally {\n-                            lock.unlock();\n                         }\n                     }\n                 } else {\n",
    "project": "jackrabbit-oak"
}