{
    "bug_id": 49,
    "classification": {
        "singleLine": false
    },
    "commit": "5594b2e0",
    "failing_tests": [
        "org.apache.accumulo.server.client.BulkImporterTest"
    ],
    "files": 6,
    "jira_id": "412",
    "linesAdd": 93,
    "linesRem": 69,
    "nb_error": 0,
    "nb_failure": 1,
    "nb_skipped": 0,
    "nb_test": 79,
    "patch": "diff --git a/src/core/src/main/java/org/apache/accumulo/core/conf/Property.java b/src/core/src/main/java/org/apache/accumulo/core/conf/Property.java\nindex 4f95e1a6c..83283ac18 100644\n--- a/src/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n+++ b/src/core/src/main/java/org/apache/accumulo/core/conf/Property.java\n@@ -65,7 +65,6 @@\n   MASTER_RECOVERY_POOL(\"master.recovery.pool\", \"recovery\", PropertyType.STRING, \"Priority queue to use for log recovery map/reduce jobs.\"),\n   MASTER_RECOVERY_SORT_MAPREDUCE(\"master.recovery.sort.mapreduce\", \"false\", PropertyType.BOOLEAN,\n       \"If true, use map/reduce to sort write-ahead logs during recovery\"),\n-  MASTER_BULK_SERVERS(\"master.bulk.server.max\", \"4\", PropertyType.COUNT, \"The number of servers to use during a bulk load\"),\n   MASTER_BULK_RETRIES(\"master.bulk.retries\", \"3\", PropertyType.COUNT, \"The number of attempts to bulk-load a file before giving up.\"),\n   MASTER_BULK_THREADPOOL_SIZE(\"master.bulk.threadpool.size\", \"5\", PropertyType.COUNT, \"The number of threads to use when coordinating a bulk-import.\"),\n   MASTER_MINTHREADS(\"master.server.threads.minimum\", \"2\", PropertyType.COUNT, \"The minimum number of threads to use to handle incoming requests.\"),\ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java\nindex 5e82a7d1e..bb4ae64e3 100644\n--- a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitionedMapper.java\n@@ -42,14 +42,13 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapreduce.Mapper;\n-import org.apache.log4j.Logger;\n \n import com.google.common.collect.HashMultimap;\n import com.google.common.collect.Multimap;\n \n public class WikipediaPartitionedMapper extends Mapper<Text,Article,Text,Mutation> {\n   \n-  private static final Logger log = Logger.getLogger(WikipediaPartitionedMapper.class);\n+  // private static final Logger log = Logger.getLogger(WikipediaPartitionedMapper.class);\n   \n   public final static Charset UTF8 = Charset.forName(\"UTF-8\");\n   public static final String DOCUMENT_COLUMN_FAMILY = \"d\";\ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java\nindex 82af9fd52..3507108b3 100644\n--- a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/ingest/WikipediaPartitioner.java\n@@ -23,40 +23,21 @@\n import java.io.ByteArrayInputStream;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.io.StringReader;\n import java.nio.charset.Charset;\n-import java.util.HashSet;\n-import java.util.IllegalFormatException;\n-import java.util.Map.Entry;\n-import java.util.Set;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n-import org.apache.accumulo.core.data.Mutation;\n-import org.apache.accumulo.core.data.Value;\n-import org.apache.accumulo.core.security.ColumnVisibility;\n import org.apache.accumulo.examples.wikisearch.ingest.ArticleExtractor.Article;\n import org.apache.accumulo.examples.wikisearch.ingest.WikipediaInputFormat.WikipediaInputSplit;\n-import org.apache.accumulo.examples.wikisearch.normalizer.LcNoDiacriticsNormalizer;\n-import org.apache.accumulo.examples.wikisearch.protobuf.Uid;\n-import org.apache.accumulo.examples.wikisearch.protobuf.Uid.List.Builder;\n-import org.apache.commons.codec.binary.Base64;\n-import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapreduce.Mapper;\n import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n-import org.apache.log4j.Logger;\n-import org.apache.lucene.analysis.tokenattributes.TermAttribute;\n-import org.apache.lucene.wikipedia.analysis.WikipediaTokenizer;\n-\n-import com.google.common.collect.HashMultimap;\n-import com.google.common.collect.Multimap;\n \n public class WikipediaPartitioner extends Mapper<LongWritable,Text,Text,Article> {\n   \n-  private static final Logger log = Logger.getLogger(WikipediaPartitioner.class);\n+  // private static final Logger log = Logger.getLogger(WikipediaPartitioner.class);\n   \n   public final static Charset UTF8 = Charset.forName(\"UTF-8\");\n   public static final String DOCUMENT_COLUMN_FAMILY = \"d\";\ndiff --git a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/output/SortingRFileOutputFormat.java b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/output/SortingRFileOutputFormat.java\nindex d8c57c2f9..2738e2c0b 100644\n--- a/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/output/SortingRFileOutputFormat.java\n+++ b/src/examples/wikisearch/ingest/src/main/java/org/apache/accumulo/examples/wikisearch/output/SortingRFileOutputFormat.java\n@@ -4,20 +4,18 @@\n \n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.data.Mutation;\n-import org.apache.accumulo.examples.wikisearch.ingest.WikipediaMapper;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.OutputCommitter;\n import org.apache.hadoop.mapreduce.OutputFormat;\n import org.apache.hadoop.mapreduce.RecordWriter;\n import org.apache.hadoop.mapreduce.TaskAttemptContext;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.io.Text;\n-import org.apache.log4j.Logger;\n \n public class SortingRFileOutputFormat extends OutputFormat<Text,Mutation> {\n \n-  private static final Logger log = Logger.getLogger(SortingRFileOutputFormat.class);\n+  // private static final Logger log = Logger.getLogger(SortingRFileOutputFormat.class);\n \n   public static final String PATH_NAME = \"sortingrfileoutputformat.path\";\n   public static final String MAX_BUFFER_SIZE = \"sortingrfileoutputformat.max.buffer.size\";\ndiff --git a/src/server/src/main/java/org/apache/accumulo/server/client/BulkImporter.java b/src/server/src/main/java/org/apache/accumulo/server/client/BulkImporter.java\nindex 071b8bd26..4ee5371f2 100644\n--- a/src/server/src/main/java/org/apache/accumulo/server/client/BulkImporter.java\n+++ b/src/server/src/main/java/org/apache/accumulo/server/client/BulkImporter.java\n@@ -38,8 +38,8 @@\n import org.apache.accumulo.core.client.Instance;\n import org.apache.accumulo.core.client.impl.ServerClient;\n import org.apache.accumulo.core.client.impl.TabletLocator;\n-import org.apache.accumulo.core.client.impl.Translator;\n import org.apache.accumulo.core.client.impl.TabletLocator.TabletLocation;\n+import org.apache.accumulo.core.client.impl.Translator;\n import org.apache.accumulo.core.client.impl.thrift.ClientService;\n import org.apache.accumulo.core.client.impl.thrift.ThriftTableOperationException;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n@@ -150,7 +150,7 @@ public void run() {\n             } catch (Exception ex) {\n               log.warn(\"Unable to find tablets that overlap file \" + mapFile.toString());\n             }\n-            \n+            log.debug(\"Map file \" + mapFile + \" found to overlap \" + tabletsToAssignMapFileTo.size() + \" tablets\");\n             if (tabletsToAssignMapFileTo.size() == 0) {\n               List<KeyExtent> empty = Collections.emptyList();\n               completeFailures.put(mapFile, empty);\n@@ -652,33 +652,41 @@ public String toString() {\n     return findOverlappingTablets(acuConf, fs, locator, file, start, failed.getEndRow());\n   }\n   \n+  final static byte[] byte0 = {0};\n+\n   public static List<TabletLocation> findOverlappingTablets(AccumuloConfiguration acuConf, FileSystem fs, TabletLocator locator, Path file, Text startRow,\n       Text endRow) throws Exception {\n     List<TabletLocation> result = new ArrayList<TabletLocation>();\n-    \n     Collection<ByteSequence> columnFamilies = Collections.emptyList();\n-    \n-    FileSKVIterator reader = FileOperations.getInstance().openReader(file.toString(), true, fs, fs.getConf(), acuConf);\n+    String filename = file.toString();\n+    // log.debug(filename + \" finding overlapping tablets \" + startRow + \" -> \" + endRow);\n+    FileSKVIterator reader = FileOperations.getInstance().openReader(filename, true, fs, fs.getConf(), acuConf);\n     try {\n       Text row = startRow;\n       if (row == null)\n         row = new Text();\n       while (true) {\n+        // log.debug(filename + \" Seeking to row \" + row);\n         reader.seek(new Range(row, null), columnFamilies, false);\n-        if (!reader.hasTop())\n+        if (!reader.hasTop()) {\n+          // log.debug(filename + \" not found\");\n           break;\n+        }\n         row = reader.getTopKey().getRow();\n         TabletLocation tabletLocation = locator.locateTablet(row, false, true);\n+        // log.debug(filename + \" found row \" + row + \" at location \" + tabletLocation);\n         result.add(tabletLocation);\n         row = tabletLocation.tablet_extent.getEndRow();\n-        if (row != null && (endRow == null || row.compareTo(endRow) < 0))\n-          row = Range.followingPrefix(row);\n-        else\n+        if (row != null && (endRow == null || row.compareTo(endRow) < 0)) {\n+          row = new Text(row);\n+          row.append(byte0, 0, byte0.length);\n+        } else\n           break;\n       }\n     } finally {\n       reader.close();\n     }\n+    // log.debug(filename + \" to be sent to \" + result);\n     return result;\n   }\n   \ndiff --git a/src/server/src/main/java/org/apache/accumulo/server/master/tableOps/BulkImport.java b/src/server/src/main/java/org/apache/accumulo/server/master/tableOps/BulkImport.java\nindex c4a3f5091..05c353dae 100644\n--- a/src/server/src/main/java/org/apache/accumulo/server/master/tableOps/BulkImport.java\n+++ b/src/server/src/main/java/org/apache/accumulo/server/master/tableOps/BulkImport.java\n@@ -19,11 +19,15 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collection;\n import java.util.Collections;\n+import java.util.HashMap;\n import java.util.HashSet;\n-import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n import java.util.Set;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Future;\n import java.util.concurrent.LinkedBlockingQueue;\n@@ -41,12 +45,13 @@\n import org.apache.accumulo.core.client.impl.thrift.TableOperationExceptionType;\n import org.apache.accumulo.core.client.impl.thrift.ThriftTableOperationException;\n import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.conf.SiteConfiguration;\n import org.apache.accumulo.core.file.FileOperations;\n import org.apache.accumulo.core.master.state.tables.TableState;\n import org.apache.accumulo.core.security.thrift.AuthInfo;\n import org.apache.accumulo.core.util.CachedConfiguration;\n import org.apache.accumulo.core.util.Daemon;\n-import org.apache.accumulo.core.util.LoggingRunnable;\n+import org.apache.accumulo.core.util.ThriftUtil;\n import org.apache.accumulo.core.util.UtilWaitThread;\n import org.apache.accumulo.server.ServerConstants;\n import org.apache.accumulo.server.client.HdfsZooInstance;\n@@ -370,7 +375,7 @@ public LoadFiles(String tableId, String source, String bulk, String errorDir, bo\n   \n   @Override\n   public Repo<Master> call(final long tid, Master master) throws Exception {\n-    \n+    final SiteConfiguration conf = ServerConfiguration.getSiteConfiguration();\n     FileSystem fs = TraceFileSystem.wrap(org.apache.accumulo.core.file.FileUtil.getFileSystem(CachedConfiguration.getInstance(),\n         ServerConfiguration.getSiteConfiguration()));\n     List<FileStatus> files = new ArrayList<FileStatus>();\n@@ -389,42 +394,68 @@ public LoadFiles(String tableId, String source, String bulk, String errorDir, bo\n     }\n     fs.delete(writable, false);\n     \n-    // group files into N-sized chunks, send the chunks to random servers\n-    final int SERVERS_TO_USE = Math.min(ServerConfiguration.getSystemConfiguration().getCount(Property.MASTER_BULK_SERVERS), master.onlineTabletServers()\n-        .size());\n-    \n-    log.debug(\"tid \" + tid + \" using \" + SERVERS_TO_USE + \" servers\");\n-    // wait for success, repeat failures R times\n     final List<String> filesToLoad = Collections.synchronizedList(new ArrayList<String>());\n     for (FileStatus f : files)\n       filesToLoad.add(f.getPath().toString());\n     \n-    final int RETRIES = Math.max(1, ServerConfiguration.getSystemConfiguration().getCount(Property.MASTER_BULK_RETRIES));\n-    for (int i = 0; i < RETRIES && filesToLoad.size() > 0; i++) {\n-      List<Future<?>> results = new ArrayList<Future<?>>();\n-      for (List<String> chunk : groupFiles(filesToLoad, SERVERS_TO_USE)) {\n-        final List<String> attempt = chunk;\n-        results.add(threadPool.submit(new LoggingRunnable(log, new Runnable() {\n+\n+    final int RETRIES = Math.max(1, conf.getCount(Property.MASTER_BULK_RETRIES));\n+    for (int attempt = 0; attempt < RETRIES && filesToLoad.size() > 0; attempt++) {\n+      List<Future<List<String>>> results = new ArrayList<Future<List<String>>>();\n+      \n+      // Figure out which files will be sent to which server\n+      Set<TServerInstance> currentServers = Collections.synchronizedSet(new HashSet<TServerInstance>(master.onlineTabletServers()));\n+      Map<String,List<String>> loadAssignments = new HashMap<String,List<String>>();\n+      for (TServerInstance server : currentServers) {\n+        loadAssignments.put(server.hostPort(), new ArrayList<String>());\n+      }\n+      int i = 0;\n+      List<Entry<String,List<String>>> entries = new ArrayList<Entry<String,List<String>>>(loadAssignments.entrySet());\n+      for (String file : filesToLoad) {\n+        entries.get(i % entries.size()).getValue().add(file);\n+        i++;\n+      }\n+      \n+      // Use the threadpool to assign files one-at-a-time to the server\n+      for (Entry<String,List<String>> entry : entries) {\n+        if (entry.getValue().isEmpty()) {\n+          continue;\n+        }\n+        final Entry<String,List<String>> finalEntry = entry;\n+        results.add(threadPool.submit(new Callable<List<String>>() {\n           @Override\n-          public void run() {\n+          public List<String> call() {\n+            if (log.isDebugEnabled()) {\n+              log.debug(\"Asking \" + finalEntry.getKey() + \" to load \" + sampleList(finalEntry.getValue(), 10));\n+            }\n+            List<String> failures = new ArrayList<String>();\n             ClientService.Iface client = null;\n             try {\n-              client = ServerClient.getConnection(HdfsZooInstance.getInstance());\n+              client = ThriftUtil.getTServerClient(finalEntry.getKey(), conf);\n+              for (String file : finalEntry.getValue()) {\n+                List<String> attempt = Collections.singletonList(file);\n+                log.debug(\"Asking \" + finalEntry.getKey() + \" to bulk import \" + file);\n                 List<String> fail = client.bulkImportFiles(null, SecurityConstants.getSystemCredentials(), tid, tableId, attempt, errorDir, setTime);\n-              attempt.removeAll(fail);\n-              filesToLoad.removeAll(attempt);\n+                if (fail.isEmpty()) {\n+                  filesToLoad.remove(file);\n+                } else {\n+                  failures.addAll(fail);\n+                }\n+              }\n             } catch (Exception ex) {\n               log.error(ex, ex);\n             } finally {\n               ServerClient.close(client);\n             }\n+            return failures;\n           }\n-        })));\n+        }));\n       }\n-      for (Future<?> f : results)\n-        f.get();\n+      Set<String> failures = new HashSet<String>();\n+      for (Future<List<String>> f : results)\n+        failures.addAll(f.get());\n       if (filesToLoad.size() > 0) {\n-        log.debug(\"tid \" + tid + \" attempt \" + (i + 1) + \" \" + filesToLoad + \" failed\");\n+        log.debug(\"tid \" + tid + \" attempt \" + (i + 1) + \" \" + sampleList(filesToLoad, 10) + \" failed\");\n         UtilWaitThread.sleep(100);\n       }\n     }\n@@ -449,16 +480,24 @@ public void run() {\n     return new CompleteBulkImport(tableId, source, bulk, errorDir);\n   }\n   \n-  private List<List<String>> groupFiles(List<String> files, int groups) {\n-    List<List<String>> result = new ArrayList<List<String>>();\n-    Iterator<String> iter = files.iterator();\n-    for (int i = 0; i < groups && iter.hasNext(); i++) {\n-      List<String> group = new ArrayList<String>();\n-      for (int j = 0; j < Math.ceil(files.size() / (double) groups) && iter.hasNext(); j++) {\n-        group.add(iter.next());\n+  static String sampleList(Collection<?> potentiallyLongList, int max) {\n+    StringBuffer result = new StringBuffer();\n+    result.append(\"[\");\n+    int i = 0;\n+    for (Object obj : potentiallyLongList) {\n+      result.append(obj);\n+      if (i >= max) {\n+        result.append(\"...\");\n+        break;\n+      } else {\n+        result.append(\", \");\n       }\n-      result.add(group);\n+      i++;\n     }\n-    return result;\n+    if (i < max)\n+      result.delete(result.length() - 2, result.length());\n+    result.append(\"]\");\n+    return result.toString();\n   }\n+\n }\n",
    "project": "accumulo",
    "buggy_compile_success": false,
    "buggy_test_success": false,
    "fixed_compile_success": false,
    "fixed_test_success": false
}